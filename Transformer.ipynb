{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e994add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74eed6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-2 configurations\n",
    "GPT_CONFIG_124= {\n",
    "    'vocab_size' :50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim':768,\n",
    "    'n_heads':12,\n",
    "    'n_layers':12,\n",
    "    'drop_rate':0.1,\n",
    "    'qkv_bias':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emd_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emd_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emd_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim =True)\n",
    "        var = x.var(dim =-1, keepdim = True, unbiased= False)\n",
    "        norm_x = (x-mean)/(torch.sqrt(var+self.eps))\n",
    "        return self.scale*norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU Activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.0447*torch.pow(x,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c16911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']), #incresing dimesion\n",
    "                                    GELU(),\n",
    "                                    nn.Linear(4*cfg['emb_dim'],cfg['emb_dim']))  #coming back to original dimension\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2add93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads==0),\\\n",
    "            \"d_out must be divisible by nums head\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_head = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "        # self.d_in = d_in\n",
    "        self.w_query = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,num_token,d_in = x.shape\n",
    "\n",
    "        keys = self.w_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        keys = keys.view(b,num_token,self.num_head,self.head_dim)\n",
    "        queries = queries.view(b,num_token,self.num_head,self.head_dim)\n",
    "        values = values.view(b,num_token,self.num_head,self.head_dim)\n",
    "\n",
    "        #grouping by num_heads\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "\n",
    "        # calculating attention score\n",
    "        attn_score = queries @ keys.transpose(2,3)\n",
    "\n",
    "        # calculating attention weigths,masking, scaling and dropout\n",
    "        mask_bool = self.mask.bool()[:num_token,:num_token]\n",
    "        attn_score= attn_score.masked_fill_(mask_bool, - torch.inf)\n",
    "        attn_weight = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "        \n",
    "        #calculating the context vector\n",
    "        context_vector = attn_weight @ values #ntokn x ntoken * ntoken x head_dim\n",
    "        # trasposing to get all the context vextor togeth\n",
    "        context_vector = context_vector.transpose(1,2)\n",
    "\n",
    "        # combining heads \n",
    "        context_vector = context_vector.contiguous().view(b,num_token,self.d_out)\n",
    "        context_vector = self.out_proj(context_vector) # optional projection\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3ab4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], d_out= cfg['emb_dim'],\n",
    "                                       context_length=cfg['context_length'],\n",
    "                                       num_heads=cfg['n_heads'], dropout= cfg['drop_rate'],\n",
    "                                       qkv_bias= cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg=cfg)\n",
    "        self.norm1 = LayerNorm(emd_dim=cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(emd_dim=cfg['emb_dim'])\n",
    "\n",
    "        self.drop_shortcut  = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x= self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        x = x+ shortcut\n",
    "\n",
    "        shortcut =x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9c2f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intput shape:  torch.Size([2, 4, 768])\n",
      "output shape:  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# testing this transformer block\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2,4,768)\n",
    "transformer = TransformerBlock(GPT_CONFIG_124)\n",
    "output = transformer(x)\n",
    "print(\"intput shape: \",x.shape)\n",
    "print(\"output shape: \",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd2fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1648,  0.4002, -0.0749,  ...,  1.2646,  0.3324,  0.7243],\n",
      "         [ 0.0293,  0.0498,  0.2529,  ...,  0.4698,  0.1281,  0.9749],\n",
      "         [ 0.5532,  0.5788, -0.0310,  ...,  1.1544,  0.3947,  0.7600],\n",
      "         [ 0.1631,  0.7128,  0.7271,  ...,  0.3312,  0.5730,  0.9258]],\n",
      "\n",
      "        [[ 0.1787,  1.1682,  0.5810,  ...,  0.1828,  0.0073, -0.5603],\n",
      "         [-0.2920,  0.6318,  0.2002,  ...,  0.3218,  0.4670, -0.0383],\n",
      "         [ 0.9275,  0.4203,  0.3183,  ...,  0.3771,  0.7190, -0.1205],\n",
      "         [ 0.6035,  0.5767,  0.3411,  ...,  1.3798,  1.2683,  0.3916]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a609c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
